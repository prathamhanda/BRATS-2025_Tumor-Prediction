#!/usr/bin/env python3
"""
ğŸ”¥ GPU Validation Script for SSA Brain Tumor Segmentation
=========================================================

This script validates your NVIDIA GeForce GTX 1650 setup and provides
optimization recommendations for intensive deep learning tasks.

Date: September 7, 2025
"""

import torch
import gc
import psutil
import platform
import subprocess
import sys

def get_gpu_info():
    """Get detailed GPU information"""
    print("ğŸ”¥ GPU VALIDATION REPORT")
    print("=" * 60)
    
    # Basic CUDA info
    cuda_available = torch.cuda.is_available()
    print(f"âœ… CUDA Available: {cuda_available}")
    
    if cuda_available:
        print(f"âœ… CUDA Version: {torch.version.cuda}")
        print(f"âœ… PyTorch Version: {torch.__version__}")
        print(f"âœ… GPU Device: {torch.cuda.get_device_name(0)}")
        
        # GPU Memory info
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"âœ… GPU Memory: {gpu_memory:.1f} GB")
        
        # Compute capability
        compute_cap = torch.cuda.get_device_properties(0)
        print(f"âœ… Compute Capability: {compute_cap.major}.{compute_cap.minor}")
        
        # Current memory usage
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            reserved = torch.cuda.memory_reserved(0) / (1024**3)
            print(f"ğŸ“Š Memory Allocated: {allocated:.2f} GB")
            print(f"ğŸ“Š Memory Reserved: {reserved:.2f} GB")
            print(f"ğŸ“Š Memory Available: {gpu_memory - reserved:.2f} GB")
            
        return True, gpu_memory, compute_cap
    else:
        print("âŒ CUDA not available - using CPU only")
        return False, 0, None

def validate_environment():
    """Validate the complete environment"""
    print("\nğŸ–¥ï¸ SYSTEM ENVIRONMENT")
    print("=" * 60)
    
    # System info
    print(f"ğŸ’» Platform: {platform.platform()}")
    print(f"ğŸ Python Version: {sys.version}")
    
    # Memory info
    memory = psutil.virtual_memory()
    print(f"ğŸ§  System RAM: {memory.total / (1024**3):.1f} GB")
    print(f"ğŸ§  Available RAM: {memory.available / (1024**3):.1f} GB")
    
    # Check required packages
    print(f"\nğŸ“¦ REQUIRED PACKAGES")
    print("=" * 60)
    
    required_packages = {
        'torch': torch.__version__,
        'numpy': None,
        'nibabel': None,
        'scipy': None,
        'scikit-image': None,
        'matplotlib': None,
        'tqdm': None
    }
    
    for package, version in required_packages.items():
        try:
            if package == 'torch':
                print(f"âœ… {package}: {version}")
            else:
                __import__(package)
                if package == 'numpy':
                    import numpy as np
                    print(f"âœ… {package}: {np.__version__}")
                elif package == 'nibabel':
                    import nibabel as nib
                    print(f"âœ… {package}: {nib.__version__}")
                elif package == 'scipy':
                    import scipy
                    print(f"âœ… {package}: {scipy.__version__}")
                elif package == 'scikit-image':
                    import skimage
                    print(f"âœ… {package}: {skimage.__version__}")
                elif package == 'matplotlib':
                    import matplotlib
                    print(f"âœ… {package}: {matplotlib.__version__}")
                elif package == 'tqdm':
                    import tqdm
                    print(f"âœ… {package}: {tqdm.__version__}")
        except ImportError:
            print(f"âŒ {package}: Not installed")

def test_gpu_operations():
    """Test basic GPU operations"""
    if not torch.cuda.is_available():
        print("\nâŒ GPU not available - skipping GPU tests")
        return False
        
    print(f"\nğŸ§ª GPU PERFORMANCE TESTS")
    print("=" * 60)
    
    try:
        # Test tensor operations
        device = torch.device('cuda:0')
        
        # Small tensor test
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        z = torch.matmul(x, y)
        print("âœ… Basic tensor operations: PASSED")
        
        # Memory allocation test
        large_tensor = torch.randn(2000, 2000, device=device)
        print("âœ… Large tensor allocation: PASSED")
        
        # 3D convolution test (important for brain tumor segmentation)
        conv3d = torch.nn.Conv3d(4, 32, kernel_size=3, padding=1).to(device)
        test_input = torch.randn(1, 4, 64, 64, 64, device=device)
        output = conv3d(test_input)
        print("âœ… 3D Convolution operations: PASSED")
        
        # Memory cleanup
        del x, y, z, large_tensor, conv3d, test_input, output
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ GPU test failed: {e}")
        return False

def recommend_optimizations(gpu_memory):
    """Provide optimization recommendations for GTX 1650"""
    print(f"\nğŸš€ OPTIMIZATION RECOMMENDATIONS FOR GTX 1650")
    print("=" * 60)
    
    if gpu_memory > 0:
        print(f"ğŸ¯ Your GTX 1650 has {gpu_memory:.1f} GB VRAM")
        
        if gpu_memory >= 4.0:
            print("âœ… Good! You have sufficient VRAM for brain tumor segmentation")
            print("ğŸ“Š Recommended settings:")
            print("   - Batch Size: 1-2 (for 128Â³ patches)")
            print("   - Mixed Precision: Enabled (save ~30% memory)")
            print("   - Gradient Checkpointing: Enabled")
            print("   - Pin Memory: True")
        else:
            print("âš ï¸ Limited VRAM - use aggressive memory optimization")
            print("ğŸ“Š Recommended settings:")
            print("   - Batch Size: 1 only")
            print("   - Mixed Precision: REQUIRED")
            print("   - Gradient Checkpointing: REQUIRED")
            print("   - Smaller patch size: Consider 96Â³ instead of 128Â³")
            
        print("\nğŸ”§ Performance Optimizations:")
        print("   - Use DataLoader with num_workers=2-4")
        print("   - Enable torch.backends.cudnn.benchmark=True")
        print("   - Use torch.compile() for PyTorch 2.0+")
        print("   - Implement gradient accumulation for larger effective batch size")
        
    print("\nğŸ’¡ Memory Management Tips:")
    print("   - Clear cache regularly: torch.cuda.empty_cache()")
    print("   - Use context managers for temporary tensors")
    print("   - Monitor memory usage during training")
    print("   - Use CPU for data preprocessing when possible")

def main():
    """Main validation function"""
    print("ğŸ”¥ Starting GPU Validation for SSA Brain Tumor Segmentation")
    print("=" * 70)
    
    # Get GPU info
    gpu_available, gpu_memory, compute_cap = get_gpu_info()
    
    # Validate environment
    validate_environment()
    
    # Test GPU operations
    if gpu_available:
        test_success = test_gpu_operations()
        
        if test_success:
            print(f"\nğŸ‰ GPU VALIDATION SUCCESSFUL!")
            print("=" * 60)
            print("âœ… Your NVIDIA GeForce GTX 1650 is ready for:")
            print("   - 3D Brain Tumor Segmentation")
            print("   - Deep Learning Training")
            print("   - Intensive Medical Image Processing")
            
            # Provide recommendations
            recommend_optimizations(gpu_memory)
            
        else:
            print(f"\nâš ï¸ GPU validation failed - check CUDA installation")
    else:
        print(f"\nâŒ GPU not available - will use CPU only (slower)")
    
    print(f"\nğŸ“‹ NEXT STEPS:")
    print("=" * 60)
    print("1. ğŸ“Š Run SSA dataset analysis")
    print("2. ğŸ”„ Execute GPU-optimized preprocessing")
    print("3. ğŸ§  Train 3D U-Net with GPU acceleration")
    print("4. ğŸ“ˆ Monitor GPU utilization during training")

if __name__ == "__main__":
    main()
